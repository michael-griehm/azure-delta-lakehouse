{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=\"my-simple-azure-keyvault-scope\",key=\"delta-lakehouse-dbx-crypto-job-runner-secret\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.dltalakehousebronze.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.dltalakehousebronze.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.dltalakehousebronze.dfs.core.windows.net\", \"53df3811-1549-4f9b-9568-8c67829cd08a\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.dltalakehousebronze.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.dltalakehousebronze.dfs.core.windows.net\", \"https://login.microsoftonline.com/f4cb4c38-d7d4-4c0f-888c-05e0ce4d7437/oauth2/token\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.dltalakehousesilver.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.dltalakehousesilver.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.dltalakehousesilver.dfs.core.windows.net\", \"53df3811-1549-4f9b-9568-8c67829cd08a\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.dltalakehousesilver.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.dltalakehousesilver.dfs.core.windows.net\", \"https://login.microsoftonline.com/f4cb4c38-d7d4-4c0f-888c-05e0ce4d7437/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Day Month Year\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today = datetime.utcnow()\n",
    "year = today.year\n",
    "month = today.month\n",
    "day = today.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive data load for all files from a day from every partition in the Event Hub Namespace\n",
    "sourcefolderpath = f\"abfss://crypto-stream@dltalakehousebronze.dfs.core.windows.net/ehns-quote-streams/eh-crypto-stream/*/{year}/{month:0>2d}/{day:0>2d}\"\n",
    "\n",
    "df = spark.read.option(\"recursiveFileLookup\",\"true\").option(\"header\",\"true\").format(\"avro\").load(sourcefolderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Body field from Binary to JSON \n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField, LongType, TimestampType\n",
    "\n",
    "sourceSchema = StructType([\n",
    "        StructField(\"Symbol\", StringType(), False),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"VolumeLastHourUSD\", DoubleType(), True),\n",
    "        StructField(\"SymbolsCount\", LongType(), True),\n",
    "        StructField(\"TradeCount\", LongType(), True),\n",
    "        StructField(\"QuoteCount\", LongType(), True),\n",
    "        StructField(\"PriceTimeStamp\", TimestampType(), True)])\n",
    "\n",
    "df = df.withColumn(\"StringBody\", col(\"Body\").cast(\"string\"))\n",
    "jsonOptions = {\"dateFormat\" : \"yyyy-MM-dd HH:mm:ss.SSS\"}\n",
    "df = df.withColumn(\"JsonBody\", from_json(df.StringBody, sourceSchema, jsonOptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the Body JSON field into columns of the DataFrame\n",
    "for c in df.schema[\"JsonBody\"].dataType:\n",
    "    df = df.withColumn(c.name, col(\"JsonBody.\" + c.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 0 priced assets\n",
    "df = df.filter(\"Price > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data\n",
    "df = df.sort(\"Symbol\", \"PriceTimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the meaningful columns for the export to Silver data zone\n",
    "exportDF = df.select(\"Symbol\", \"Price\", \"Name\", \"VolumeLastHourUSD\", \"SymbolsCount\", \"TradeCount\", \"QuoteCount\", \"PriceTimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Price to Decimal\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "exportDF = exportDF.withColumn(\"Price\", df.Price.cast(DecimalType(38,15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the parquet file in the bronze crypto data zone\n",
    "manualpartitionfolderpath = f\"abfss://crypto-data@dltalakehousesilver.dfs.core.windows.net/quotes-by-day/{year}/{month:0>2d}/{day:0>2d}\"\n",
    "\n",
    "exportDF.write.mode(\"overwrite\").parquet(manualpartitionfolderpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b239acf2821489c398a9848859e84ce39b99d30cc4031fb37cc7461da3883639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
